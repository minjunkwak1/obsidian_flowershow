  

### ê° Pseudo grainì—ì„œ ì ìš©í•˜ê¸° ìœ„í•œ NN ë§Œë“¤ê¸°

- ê° PGì—ì„œ fiber volume fractionì€ ëª¨ë‘ ë™ì¼í•˜ë‹¤
- ê° pseudo grainì—ì„œì˜ Cê°’ì€ í•˜ë‚˜ì˜ ê³µí†µëœ C_effë¥¼ ì´ìš©í•˜ì—¬ rotationì„ í†µí•´ ê³„ì‚°ë¨
- ëª¨ë“  elementì™€ PGì— ëŒ€í•´ì„œ C_eff ê°’ì€ ê°™ê³ , ê° PGì˜ basis direction, weight fractionì´ ë‹¤ë¦„.

  

  

### ì íƒ„-ì ì†Œì„± NN parameter

[[Jordan et al., 2020]]

linear elastic elementëŠ” temperature dependent partë¥¼ ê³ ë ¤í•œ ì‹ìœ¼ë¡œ

![[Untitled 57.png|Untitled 57.png]]

  

viscoelastic elementëŠ” NN ì ìš©

ì´ ê²½ìš° inputìœ¼ë¡œ strain, strain rate, temperature

hardeningì˜ ê²½ìš°ì—ë„ neural networkë¡œ í•´ê²°í•˜ë ¤ê³  í•¨. PPì˜ strain 0.6ê¹Œì§€ëŠ” ì˜ ì„¤ëª…í–ˆë‹¤ê³  í•¨.

  

### Parameter ë³€ê²½ ê°€ëŠ¥í•œ NN

[[Koric et al., 2023]]

Von Mises + isotropic hardeningì„ ê°€ì •

input functionì„ matching output functionì— ë§¤í•‘í•˜ëŠ” ë°©ì‹.

![[Untitled 1 23.png|Untitled 1 23.png]]

  

  

[[Lu et al., 2021]]

![[Untitled 2 21.png|Untitled 2 21.png]]

a, For the network to learn an operator G : u â†¦ G(u) it takes two inputs [u(x1), u(x2), ..., u(xm)] and y.

b, Illustration of the training data. For each input function u, we require that we have the same number of evaluations at the same scattered sensors x1, x2, ..., xm. however, we do not enforce any constraints on the number or locations for the evaluation of output functions.

c, The stacked DeepONet is inspired by Theorem 1, and has one trunk network and p stacked branch networks. The network constructed in Theorem 1 is a stacked DeepONet formed by choosing the trunk net as a one-layer network of width p and each branch net as a one-hidden-layer network of width n.

**d, The unstacked DeepONet is inspired by Theorem 2, and has one trunk network and one branch network. An unstacked DeepONet can be viewed as a stacked DeepONet with all the branch nets sharing the same set of parameters.**

  

  

universal operator approximation theoremì— ê·¼ê±°í•¨.

â†’ single layer NNìœ¼ë¡œ ëª¨ë“  ë¹„ì„ í˜• ì—°ì† í•¨ìˆ˜(a mapping from a space of functions into real numbers)ì™€ operator (a mapping from a space of functions into another space of functions) ê·¼ì‚¬ ê°€ëŠ¥

â†’ Gê°€ operator, uê°€ input function, G(u)ê°€ output function, point yì— ëŒ€í•´ G(u)(y)ëŠ” real number.

**figure a** ì—ì„œ inputìœ¼ë¡œ ë“¤ì–´ê°€ëŠ”ê²Œ uì™€ yì´ë©° outputì´ G(u)(y)

ì´ ë–„ input functionì„ discretely ì£¼ì–´ì„œ network approximationì´ ê°€ëŠ¥í•˜ê²Œ í•¨.

â€˜sensorâ€™ëŠ” input space Vì—ì„œ {x1,..,xm}ì—ì„œì˜ í•¨ìˆ˜ê°’ìœ¼ë¡œ ì´ë£¨ì–´ì§„ ê°€ì¥ ê°„ë‹¨í•œ function?ì„ ì˜ë¯¸

  

DeepPNetì€ offline trainingì„ ê±°ì¹˜ê²Œ ë¨. ì´ ë•Œ DNN í›ˆë ¨ì„ ìœ„í•´ classical numerical methodë¥¼ í™œìš©í•˜ì—¬, target operatorë¥¼ í’€ê²Œ ë¨.

  

DONETì˜ êµ¬ì¡°

trunk network â†’ yë¥¼ inputìœ¼ë¡œ ë°›ì•„ì„œ outputs [t1,t2, ..., tp]T âˆˆ Rp

branch network â†’ [u(x1),u(x2), ..., u(xm)]T ì„ inputìœ¼ë¡œ ë°›ì•„ì„œ scalar bk âˆˆ R for k = 1, 2, ..., p. ê°’ì„outputìœ¼ë¡œ ë°›ìŒ.

![[Untitled 3 18.png|Untitled 3 18.png]]

í•´ë‹¹ modelì´ python library DeepXDEì— implemented.

  

  

## ë¯¸íŒ…ìë£Œ ë‚´ìš© êµ¬ì„±

1. í•´ê²°í•  ë¬¸ì œë“¤
    1. NN for NR
    2. ì—¬ëŸ¬ material propertyë¥¼ ëª¨ë‘ ê³ ë ¤í•  ìˆ˜ ìˆëŠ” NN
2. ì¼ë°˜ì ì¸ NN for NR
    1. ëŒ€ë¶€ë¶„ systemì´ í•œì •ë˜ì–´ ìˆìŒ
3. transfer learning
    1. material propertyê°€ ë³€ê²½ë˜ëŠ” ê²½ìš° ë¹ ë¥´ê²Œ retrainingì´ ê°€ëŠ¥í•  ê²ƒ
4. DeepONET
    1. operator trainingì„ í†µí•´ ì—¬ëŸ¬ material ì»¤ë²„ ê°€ëŠ¥
5. ì…ë ¥ í•„ìš”í•œ parameter
    
      
    
      
    

3í˜ì´ì§€

ì—¬ëŸ¬ systemì„ ì»¤ë²„í•  ìˆ˜ ìˆìœ¼ë ¤ë©´ ëª¨ë“  material parameterì— ëŒ€í•´ inputìœ¼ë¡œ ê³ ë ¤í•œ neural networkì´ í•„ìš”í•œë°, ì´ë ‡ê²Œ ì¸ê³µì‹ ê²½ë§ì„ í•™ìŠµí•œ ê²½ìš°ëŠ” ê±°ì˜ ì—†ì—ˆìŒ.

ì´ëŠ” ëª¨ë“  material parameterë¥¼ inputìœ¼ë¡œ ë„£ê¸°ì— ë„ˆë¬´ ë§ì€ costê°€ ë“¤ì–´ê°€ê±°ë‚˜ í•™ìŠµ í›„ ì•ˆì •ì„±ì´ ë–¨ì–´ì¡Œê¸° ë•Œë¬¸ì´ë¼ê³  ìƒê°ë¨

ê·¸ë˜ì„œ ì¼ë°˜ì ìœ¼ë¡œëŠ” ìƒˆë¡œìš´ material parameterë¥¼ ë„ì…í•˜ê¸° ìœ„í•´ì„  ë‹¤ì‹œ í•™ìŠµì„ ì‹œì¼œì•¼ í•˜ëŠ”ë°, ì´ ë¶€ë¶„ì—ì„œ ì¸ê³µì‹ ê²½ë§ì„ êµ¬ì„±í•  ë•Œ transfer learningì´ë¼ëŠ” ê°œë…ì´ ë„ì…ë˜ì–´ ì“°ê¸°ë„ í•¨.

transfer learningì€ ê¸°ì¡´ í•™ìŠµí•œ neural networkì˜ weight ì™€ biasê°’ì„ í™œìš©í•´ ìƒˆë¡œìš´ ì¸ê³µì‹ ê²½ë§ í›ˆë ¨ì— í™œìš©í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ, ë” ì ì€ datasetìœ¼ë¡œë„ ì•ˆì •ì ìœ¼ë¡œ í•™ìŠµì´ ê°€ëŠ¥í•˜ê±°ë‚˜, ë” ë¹ ë¥´ê²Œ í•™ìŠµì´ ê°€ëŠ¥í•œ ì¥ì ì„ ê°–ê²Œ ë¨.

ì´ transfer learning ê°œë…ì„ êµ¬ì„±ë°©ì •ì‹ì— ì ìš©í•´ì„œ ë‹¤ë¥¸ parameterë¥¼ ê°–ëŠ” material ì˜ í•™ìŠµì— í™œìš©í•œ ì‚¬ë¡€ë¥¼ ì°¾ì„ ìˆ˜ ìˆì—ˆìŒ. ì—¬ê¸°ì„œëŠ” ìƒˆë¡œìš´ material parameterë¥¼ ì ìš©í•œ datasetì„ transfer learningìœ¼ë¡œ í•™ìŠµì‹œì¼°ì„ ë•Œ ë” ì ì€ datasetìœ¼ë¡œ ë” ë¹ ë¥´ê²Œ í•™ìŠµì‹œí‚¬ ìˆ˜ ìˆì—ˆìŒ

ì˜¤ë¥¸ìª½ì€ transfer learning ê°œë…ì„ í™œìš©í•´ì„œ í•˜ë‚˜ì˜ deep material network databaseë¥¼ êµ¬ì¶•í•œ ì‚¬ë¡€. SFRPì—ì„œ fiberì˜ orientationê³¼ fiber volume fractionì„ ë‹¤ë¥´ê²Œ í•œ RVE 4ê°œë¥¼ êµ¬ì„±í•˜ê³ , ê° RVEë¥¼ ê·¸ëŒ€ë¡œ í•™ìŠµí•˜ëŠ” ê²ƒì´ ì•„ë‹ˆë¼ transfer learning ê°œë…ì„ í™œìš©í•´ì„œ í•™ìŠµí•œ ë’¤ í•˜ë‚˜ì˜ databaseë¥¼ êµ¬ì¶•í–ˆì„ ë•Œ parameterê°„ì˜ interpolationì„ í†µí•´ì„œ SFRPì—ì„œ ëª¨ë“  orientationê³¼ fiber volume fractionì„ ì»¤ë²„í•  ìˆ˜ ìˆëŠ” ì¸ê³µì‹ ê²½ë§ì„ êµ¬ì„±í•  ìˆ˜ ìˆì—ˆìŒ. ë¶€í’ˆë‹¨ìœ„ì˜ í•´ì„ì„ ìˆ˜ ì´ˆ ì•ˆì— ê°€ëŠ¥í•˜ê²Œ í–ˆìŒ.

ìš°ë¦¬ ê³¼ì œì— ì´ ê°œë…ì„ ì ìš©í•œë‹¤ë©´ ê° Pseudograinì´ UD í˜•íƒœì´ê¸° ë•Œë¬¸ì— fiber orientationì´ ë‹¤ë¥¸ RVE ëŒ€ì‹ ì— fiber, matrixì˜ parameterê°€ ë‹¤ë¥¸ RVEë¥¼ ê°™ì€ schemeìœ¼ë¡œ í•™ìŠµì‹œì¼œì„œ í•˜ë‚˜ì˜ DMNdatabaseë¥¼ êµ¬ì¶•í•  ìˆ˜ ìˆì„ ê²ƒ.

  

4í˜ì´ì§€

  

ìµœê·¼ ë¨¸ì‹ ëŸ¬ë‹ ì—°êµ¬ì—ì„œ operatorë¥¼ ì´ìš©í•œ ì¸ê³µì‹ ê²½ë§ ê°œë…ì¸ deep learning operator networkì´ ê³ ì•ˆë˜ì–´ inputìœ¼ë¡œ ë°›ì€ functionì„ ë‹¤ë¥¸ functionìœ¼ë¡œ ë§¤í•‘í•  ìˆ˜ ìˆëŠ” operatorë¥¼ ë„ì¶œí•  ìˆ˜ ìˆëŠ” ì¸ê³µì‹ ê²½ë§ì„ êµ¬ì¶•í•  ìˆ˜ ìˆê²Œ ë¨

ê²½ëª¨í˜•ì´ ì—°êµ¬ì¤‘ì¸ correction tensorì™€ ë¹„ìŠ·í•œ ê°œë….  
GëŠ” function uë¥¼ inputìœ¼ë¡œ ë°›ëŠ” operator, G(u)ëŠ” corresponding output functionì„. ë”°ë¼ì„œ G(u)ì˜ domain ìƒì˜ ëª¨ë“  y pointì— ëŒ€í•´ì„œ G(u)(y)ëŠ” ì‹¤ìˆ˜ê°’ìœ¼ë¡œ ì¶œë ¥ë¨. ì´ ë•Œ Networkì€ inputì„ uì™€ y ë‘ ë¶€ë¶„ìœ¼ë¡œ ë‚˜ëˆ„ì–´ ë°›ì•„ í•˜ë‚˜ì˜ G(u)(y) ê°’ì„ ì¶œë ¥í•¨  

Total errorë¥¼ ì¤„ì¼ ìˆ˜ ìˆë„ë¡ fully connected network ëŒ€ì‹  functionì— ëŒ€í•œ branch networkì™€ domain ìƒì˜ pointì— ëŒ€í•œ trunk networkìœ¼ë¡œ ë‚˜ëˆ„ì–´ ì¸ê³µì‹ ê²½ë§ êµ¬ì„± í›„ í•˜ë‚˜ì˜ operatorë¡œ í†µí•©í•˜ëŠ” ë°©ì‹ì„ ì±„íƒí•¨

  

5í˜ì´ì§€

DeepONET ê°œë…ì„ isotropic hardening ë° 2D Von mises plasticityì— ì ìš©í•´ì„œ operatorë¥¼ í†µí•œ functionê°„ mappingì„ í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ trainingì— ì‚¬ìš©ë˜ì§€ ì•Šì€ material parameterë„ ìˆ˜ìš©í•  ìˆ˜ ìˆë„ë¡ í•˜ëŠ” ì‹œë„ê°€ ì´ë£¨ì–´ì§

ğœ’ âˆˆ XëŠ” ì„œë¡œ ë‹¤ë¥¸ load, elastic ë° plastic parameter inputê°’, yëŠ” 3D/2D domain ìƒ pointì˜ coordinate ê°’, s âˆˆ SëŠ” unknown stress solution ì— í•´ë‹¹í•¨. DeepONETì€ ğœ’ì™€ yê°’ì„ ì´ìš©í•´ì„œ mapping operator G âˆ¶ X â†’ Së¥¼ í•™ìŠµí•˜ê²Œ ë¨

Gê°€ í•™ìŠµë˜ê³  ë‚˜ë©´ ì„ì˜ì˜ parameterë¥¼ ê°–ëŠ” ë‹¤ë¥¸ chiê°’ì´ ë“¤ì–´ì™”ì„ ë•Œ ì´ë¥¼ stressê°’ìœ¼ë¡œ mappingí•˜ëŠ” ë°©ì‹ìœ¼ë¡œ ì„ì˜ì˜ parameterì— ëŒ€í•´ì„œ ì •í™•í•œ output ë„ì¶œì´ ê°€ëŠ¥í•˜ê²Œ ë¨.

ì„ì˜ì˜ parameter(load, direction of load, elastic modulus, yield stress)ì— ëŒ€í•´ ì ìš©í–ˆì„ ë•Œ ë†’ì€ ì •í™•ë„ë¥¼ ë³´ì—¬ì£¼ì—ˆìœ¼ë©°, conventional numerical methodì— ë¹„í•´ ê³„ì‚° ì‹œê°„ì´ ìƒë‹¹íˆ ë‹¨ì¶•ë¨