  

### 각 Pseudo grain에서 적용하기 위한 NN 만들기

- 각 PG에서 fiber volume fraction은 모두 동일하다
- 각 pseudo grain에서의 C값은 하나의 공통된 C_eff를 이용하여 rotation을 통해 계산됨
- 모든 element와 PG에 대해서 C_eff 값은 같고, 각 PG의 basis direction, weight fraction이 다름.

  

  

### 점탄-점소성 NN parameter

[[Jordan et al., 2020]]

linear elastic element는 temperature dependent part를 고려한 식으로

![[Untitled 57.png|Untitled 57.png]]

  

viscoelastic element는 NN 적용

이 경우 input으로 strain, strain rate, temperature

hardening의 경우에도 neural network로 해결하려고 함. PP의 strain 0.6까지는 잘 설명했다고 함.

  

### Parameter 변경 가능한 NN

[[Koric et al., 2023]]

Von Mises + isotropic hardening을 가정

input function을 matching output function에 매핑하는 방식.

![[Untitled 1 23.png|Untitled 1 23.png]]

  

  

[[Lu et al., 2021]]

![[Untitled 2 21.png|Untitled 2 21.png]]

a, For the network to learn an operator G : u ↦ G(u) it takes two inputs [u(x1), u(x2), ..., u(xm)] and y.

b, Illustration of the training data. For each input function u, we require that we have the same number of evaluations at the same scattered sensors x1, x2, ..., xm. however, we do not enforce any constraints on the number or locations for the evaluation of output functions.

c, The stacked DeepONet is inspired by Theorem 1, and has one trunk network and p stacked branch networks. The network constructed in Theorem 1 is a stacked DeepONet formed by choosing the trunk net as a one-layer network of width p and each branch net as a one-hidden-layer network of width n.

**d, The unstacked DeepONet is inspired by Theorem 2, and has one trunk network and one branch network. An unstacked DeepONet can be viewed as a stacked DeepONet with all the branch nets sharing the same set of parameters.**

  

  

universal operator approximation theorem에 근거함.

→ single layer NN으로 모든 비선형 연속 함수(a mapping from a space of functions into real numbers)와 operator (a mapping from a space of functions into another space of functions) 근사 가능

→ G가 operator, u가 input function, G(u)가 output function, point y에 대해 G(u)(y)는 real number.

**figure a** 에서 input으로 들어가는게 u와 y이며 output이 G(u)(y)

이 떄 input function을 discretely 주어서 network approximation이 가능하게 함.

‘sensor’는 input space V에서 {x1,..,xm}에서의 함수값으로 이루어진 가장 간단한 function?을 의미

  

DeepPNet은 offline training을 거치게 됨. 이 때 DNN 훈련을 위해 classical numerical method를 활용하여, target operator를 풀게 됨.

  

DONET의 구조

trunk network → y를 input으로 받아서 outputs [t1,t2, ..., tp]T ∈ Rp

branch network → [u(x1),u(x2), ..., u(xm)]T 을 input으로 받아서 scalar bk ∈ R for k = 1, 2, ..., p. 값을output으로 받음.

![[Untitled 3 18.png|Untitled 3 18.png]]

해당 model이 python library DeepXDE에 implemented.

  

  

## 미팅자료 내용 구성

1. 해결할 문제들
    1. NN for NR
    2. 여러 material property를 모두 고려할 수 있는 NN
2. 일반적인 NN for NR
    1. 대부분 system이 한정되어 있음
3. transfer learning
    1. material property가 변경되는 경우 빠르게 retraining이 가능할 것
4. DeepONET
    1. operator training을 통해 여러 material 커버 가능
5. 입력 필요한 parameter
    
      
    
      
    

3페이지

여러 system을 커버할 수 있으려면 모든 material parameter에 대해 input으로 고려한 neural network이 필요한데, 이렇게 인공신경망을 학습한 경우는 거의 없었음.

이는 모든 material parameter를 input으로 넣기에 너무 많은 cost가 들어가거나 학습 후 안정성이 떨어졌기 때문이라고 생각됨

그래서 일반적으로는 새로운 material parameter를 도입하기 위해선 다시 학습을 시켜야 하는데, 이 부분에서 인공신경망을 구성할 때 transfer learning이라는 개념이 도입되어 쓰기도 함.

transfer learning은 기존 학습한 neural network의 weight 와 bias값을 활용해 새로운 인공신경망 훈련에 활용하는 방식으로, 더 적은 dataset으로도 안정적으로 학습이 가능하거나, 더 빠르게 학습이 가능한 장점을 갖게 됨.

이 transfer learning 개념을 구성방정식에 적용해서 다른 parameter를 갖는 material 의 학습에 활용한 사례를 찾을 수 있었음. 여기서는 새로운 material parameter를 적용한 dataset을 transfer learning으로 학습시켰을 때 더 적은 dataset으로 더 빠르게 학습시킬 수 있었음

오른쪽은 transfer learning 개념을 활용해서 하나의 deep material network database를 구축한 사례. SFRP에서 fiber의 orientation과 fiber volume fraction을 다르게 한 RVE 4개를 구성하고, 각 RVE를 그대로 학습하는 것이 아니라 transfer learning 개념을 활용해서 학습한 뒤 하나의 database를 구축했을 때 parameter간의 interpolation을 통해서 SFRP에서 모든 orientation과 fiber volume fraction을 커버할 수 있는 인공신경망을 구성할 수 있었음. 부품단위의 해석을 수 초 안에 가능하게 했음.

우리 과제에 이 개념을 적용한다면 각 Pseudograin이 UD 형태이기 때문에 fiber orientation이 다른 RVE 대신에 fiber, matrix의 parameter가 다른 RVE를 같은 scheme으로 학습시켜서 하나의 DMNdatabase를 구축할 수 있을 것.

  

4페이지

  

최근 머신러닝 연구에서 operator를 이용한 인공신경망 개념인 deep learning operator network이 고안되어 input으로 받은 function을 다른 function으로 매핑할 수 있는 operator를 도출할 수 있는 인공신경망을 구축할 수 있게 됨

경모형이 연구중인 correction tensor와 비슷한 개념.  
G는 function u를 input으로 받는 operator, G(u)는 corresponding output function임. 따라서 G(u)의 domain 상의 모든 y point에 대해서 G(u)(y)는 실수값으로 출력됨. 이 때 Network은 input을 u와 y 두 부분으로 나누어 받아 하나의 G(u)(y) 값을 출력함  

Total error를 줄일 수 있도록 fully connected network 대신 function에 대한 branch network와 domain 상의 point에 대한 trunk network으로 나누어 인공신경망 구성 후 하나의 operator로 통합하는 방식을 채택함

  

5페이지

DeepONET 개념을 isotropic hardening 및 2D Von mises plasticity에 적용해서 operator를 통한 function간 mapping을 하는 방식으로 training에 사용되지 않은 material parameter도 수용할 수 있도록 하는 시도가 이루어짐

𝜒 ∈ X는 서로 다른 load, elastic 및 plastic parameter input값, y는 3D/2D domain 상 point의 coordinate 값, s ∈ S는 unknown stress solution 에 해당함. DeepONET은 𝜒와 y값을 이용해서 mapping operator G ∶ X → S를 학습하게 됨

G가 학습되고 나면 임의의 parameter를 갖는 다른 chi값이 들어왔을 때 이를 stress값으로 mapping하는 방식으로 임의의 parameter에 대해서 정확한 output 도출이 가능하게 됨.

임의의 parameter(load, direction of load, elastic modulus, yield stress)에 대해 적용했을 때 높은 정확도를 보여주었으며, conventional numerical method에 비해 계산 시간이 상당히 단축됨